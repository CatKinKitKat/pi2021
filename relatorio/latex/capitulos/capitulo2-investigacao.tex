\chapter{Identificação dos requisitos}
\label{cap2}
\section{Objectivo}

O objectivo deste capítulo será esclarecer alguns conceitos mais teóricos associados ao trabalho realizado. 
Assim sendo, foram obtidos todos os dados de \textit{posts} e comentários relacionados com \textit{providers} de acesso, entretenimento, refeições e estadia directamente ligados ao património cultural do Alentejo. Estes a serem analisados e classificados, criando assim um modelo de possíveis sentimentos e procuras que o comércio local tem o interesse em fornecer aos visitantes. O foco principal foram os \textit{posts} e comentários em português.

\section{\textit{Websites} escolhidos}

Os \textit{websites} seleccionados assim como os métodos de obtenção de dados, e também de analisar os mesmos foi decidida previamente pelo grupo e todos os elementos decidiram usar as mesmas ferramentas para cada função que lhes fora determinado.
Após exaustiva procura pelos \textit{websites} preferenciais e discussão entre todos os elementos do grupo, para a utilização de um conjunto deles onde fosse possível se realizar uma boa análise e obtenção dos dados, com especial foco na linguagem portuguesa, reunimos alguns possíveis candidatos que serão abordados no ponto seguinte.

Foi decidido também que dentro das opções que serão referidas em seguida, teremos preferência na utilização de um em específico, uma vez que temos intenções de dar alguma prioridade ás informações relacionadas com o turismo rural alentejano, sendo assim e devido a uma maior procura e número de resultados que o \textit{website} oferecia, inicialmente iríamos utilizar o \textit{website TripAdvisor} como primeira opção.

\newpage
\section{Sites pesquisados}

Foram usados os sites:
\begin{itemize}
  \setlength\itemsep{0.05em}
  \item \textit{\href{https://www.tripadvisor.com/}{TripAdvisor}}
  \item \textit{\href{https://www.booking.com/}{Booking}}
  \item \textit{\href{https://www.zomato.com/}{Zomato}}
  \item \textit{\href{https://developers.google.com/maps}{Google Maps}}
\end{itemize}

O foco principal é o \textit{\href{https://www.tripadvisor.com/}{TripAdvisor}} visto que este oferece a maior variedade de conteúdo (hotéis, restaurantes e outros estabelecimentos), no entanto como uma plataforma é pouco, decidimos adicionar \textit{\href{https://www.booking.com/}{Booking}} e \textit{\href{https://www.zomato.com/}{Zomato}} à lista para uma maior e mais ampla rede de hotéis e restaurantes.

Foi também considerado o \textit{Google Maps}, mas este apresentou um novo set de problemas que vão ser descritos já de seguida.

\section{(Im)possibilidade de uso de \textit{APIs}}

Em nenhum dos \textit{websites} testados foi observada uma facilidade na obtenção de acesso às suas \textit{APIs}, apenas alguns (3/4) ofereceram acesso à documentação da(s) mesma(s) facilmente. A maioria requer um contacto, que foi tentado e continuou sem resposta durante quase todo o processo de elaboração do trabalho(contactos iniciados por Amaro, entre dia 26 e 29 de Outubro, e dia 11 de Novembro de 2021).

Dentro dos \textit{websites} que oferecem documentação foi observado que todos subdividem os seus serviços de \textit{API} em 3 ou 4 \textit{APIs} para casos de uso específicos (reservas, dados, etc) em vez de uma com \textit{endpoints} que oferecem solução para todos os casos.

A anomalia aqui é o \textit{Google Maps} que é o único que facilita o acesso à \textit{API} (mas paga, temos de ver os créditos disponíveis no \textit{Cloud Platform}), e de elevada dificuldade em \textit{scraping} pelo óbvio.

\section{Alternativas}

Sendo que é impossível o uso das \textit{APIs} (que facilitariam o trabalho) temos de recorrer a outras técnicas para obter os dados.

\subsection{\textit{Web-crawling} VS \textit{web-scraping}}

\textit{Web crawling}, também conhecido como Indexação, é usado para indexar as informações na página usando bots também conhecidos como \textit{trackers}.
O \textit{tracker} é essencialmente o que os motores de busca fazem.
É uma questão de visualizar uma página como um todo e indexá-la.
Quando um \textit{Bot} rastreia um \textit{website}, ele passa por todas as páginas e todos os \textit{links}, até a última linha do \textit{website}, em busca de qualquer informação.

O \textit{web scraping}, também conhecido como extracção de dados da \textit{web}, é semelhante ao \textit{web crawling}, pois identifica e localiza os dados de destino das páginas da \textit{web}.
A principal diferença é que, com o \textit{web scraping}, sabemos quem identificou o conjunto de dados exactamente, por exemplo, uma estrutura de elemento \textit{HTML} para páginas da \textit{web} que estão a ser corrigidas, da qual os dados precisam ser extraídos.

Com isto visto, \textit{web scraping} é o nosso alvo, visto que minimiza lixo e é direccionado.
No entanto devemos olhar para:

\subsubsection{Vantagens de \textit{web scraping}}

\begin{enumerate}
  \setlength\itemsep{0.05em}
  \item Mais rápido: É possível manusear grandes quantidades de dados que poderiam levar dias ou semanas a serem processados através do trabalho manual, com o uso do \textit{scraping} podemos reduzir substancialmente o esforço e aumentar a velocidade de decisão;
  \item Confiável e consistente: Ao fazer o trabalho manual é muito fácil de haver erros, por exemplo, erros tipográficos, informações esquecidas ou inserção nas colunas erradas. O uso do \textit{web scraping} garante consistência e a qualidade dos dados;
  \item Ajuda a reduzir a carga de trabalho;
  \item Menor custo: Uma vez implementado o \textit{scraping}, o custo total da extração de dados é significativamente reduzido, especialmente quando comparado ao trabalho manual;
  \item Manutenção básica: Fazer o \textit{scraping} de dados geralmente não requer muita manutenção.
\end{enumerate}

\subsubsection{Desvantagens de \textit{web scraping}}

\begin{enumerate}
  \setlength\itemsep{0.05em}
  \item Baixa proteção: Se os dados na \textit{web} são protegidos, o uso do \textit{scraping} também pode se tornar um desafio e aumentar os custos;
  \item Dados estruturados: Não vai ser possível fazer scraping a 1000 \textit{websites} diferentes pois cada \textit{website} tem uma estrutura completamente diferente. Será necessário haver alguma estrutura básica que seja diferente em determinadas situações.
\end{enumerate}

\subsection{Necessidade de \textit{Web Scraping}}

Com o acima dito, é necessário recorrer a soluções de \textit{Web Scraping}.

A comunidade internauta reparou no mesmo, visto que em reacção ao observado existem dezenas de projectos e tutoriais de \textit{Web Scraping} das variadas plataformas de turismo e reservas.
Infelizmente, as mesmas não ajudam no processo e cada uma tem um forma de actuação bastante diferente.

\section{Bibliotecas de \textit{Python} para \textit{Web Scraping}}

Para fazer \textit{Web Scraping} vamos usar \textit{Python}, pela sua facilidade de uso e multifaceta \textit{``Development speed is more important than execution speed''}.
Com \textit{Python} também temos as opções de criar cadernos \textit{Jupyter} onde o próprio código e os resultados são ``encadernados'' com parágrafos de texto fazendo o próprio projecto o seu pequeno relatório de progresso e resultados; como também a criação de ambientes virtuais (\textit{containers}), onde os pacotes usados ficam registados e instalados localmente, garantindo assim a portabilidade.

Para tal linguagem existem 5 grandes bibliotecas para a resolução deste caso:
\begin{itemize}
  \setlength\itemsep{0.05em}
  \item \textit{\href{https://pypi.org/project/requests/}{Requests}}
  \item \textit{\href{https://pypi.org/project/BeautifulSoup/}{BeautifulSoup}}
  \item \textit{\href{https://pypi.org/project/Scrapy3/}{Scrapy}}
  \item \textit{\href{https://pypi.org/project/lxml/}{lxml}}
\end{itemize}

Cada uma tem diferentes vantagens e desvantagens.
Caso nenhuma destas tivesse resultado, teríamos usado \href{https://pypi.org/project/selenium/}{Selenium}, que é uma biblioteca mais completa e poderosa que as listadas, visto que é uma ferramenta de testes de automação.
Porém tem um nível de complexidade maior e requer um \textit{setup} inicial maior e mais trabalhoso, requer \textit{WebDrivers} para a execução das tarefas, pode ser complicada com Firefox, sendo preferencial usar \textit{Chromium-based Browsers} como o Chrome e o novo Edge (necessário ainda o \textit{ChromeDriver} e o \textit{EdgeDriver})..
Tentaremos evitar essa, a todo o custo, pela sua complexidade e extras desnecessários às nossas necessidades e custo temporal do setup inicial.

Para cada website pode ser necessário usar bibliotecas diferentes por necessidade ou por obtenção de informações/blog posts/etc\ldots que facilitem ou melhorem o output desejado.

\subsection{Tratamento de output}

Os outputs do conteúdo \textit{scraped} podem vir em \textit{.xml} ou \textit{.csv} (ou outras mas essencialmente essas duas).
Tentámos ao máximo usar \textit{.csv}, e transformar qualquer \textit{.xml} em \textit{.csv}, visto que um maior número de ferramentas gráficas (Excel, PowerBI, etc\ldots) e/ou bibliotecas de \textit{Python (Pandas, matplotlib, etc)} para a análise de dados tratam melhor ficheiros separados por vírgulas.

Foi também feita uma análise e extração de \textit{keywords} nos textos das descrições e \textit{reviews}.
Para tal existem variados algoritmos que podemos usar, alguns "clássicos" outros até de \textit{machine learning}.

Inicialmente todas as informações (dados e meta-dados) são dados como relevantes, após consideração e ponderação durante análises iniciais do decorrer do estudo poderemos descartar dados que não consideremos relevantes.
No entanto nada nos impede de tentar prever ou imaginar quais esses serão e posteriormente avaliar o nosso julgamento para ver o que foi aprendido.

\section{Análise}
\subsection{Algoritmos de mineração de texto}

Os algoritmos de análise de texto podem ser considerados ferramentas de mineração de texto, isto é, o processo de descoberta de conhecimento potencialmente útil e inicialmente desconhecido, ou seja, a extracção de conhecimento útil utilizando bases textuais.

O processo de mineração de texto é dividido em quatro etapas bem definidas:
\begin{itemize}
  \setlength\itemsep{0.05em}
  \item Selecção;
  \item Pré-processamento;
  \item Mineração;
  \item Assimilação.
\end{itemize}

Na selecção, os documentos relevantes devem ser escolhidos e mais tarde processados.
No pré-processamento ocorrerá a conversão dos documentos em uma estrutura compatível com minerador, bem como ocorrerá um tratamento especial do texto.
Na mineração, o minerador irá detectar os padrões com base no algoritmo escolhido.
E por fim, na assimilação, os utilizadores irão utilizar o conhecimento gerado para apoiar as suas decisões. 

Por outras palavras este processo de mineração de texto pode também ser chamado de \textit{webscrapping} que é o utilizado no nosso trabalho.

A etapa pré-processamento pode ser dividida em quatro tarefas:
\begin{itemize}
  \setlength\itemsep{0.05em}
  \item Remover \textit{stopwords};
  \item Compilação;
  \item Normalização de sinónimos;
  \item Indexação.
\end{itemize}

Na etapa de remoção de \textit{stopwords} os termos com pouca ou nenhuma relevância para o documento serão removidos.
São palavras auxiliares ou conectivas, ou seja, não são discriminantes para o conteúdo do documento.

Na etapa seguinte, compilação, realiza-se uma normalização morfológica, ou seja, as palavras são reduzidas ao seu radical, serão combinadas em uma única representação. A radicalização pode ser efectuada com o auxílio de algoritmos de radicalização, sendo os mais utilizados o algoritmo de \textit{Porter (Porter Stemming Algorithm) e algoritmo de Orengo (Stemmer Portuguese ou RLSP)}.

Após a compilação, na etapa de normalização de sinónimos, os termos que possuem significados similares serão agrupados em um único termo, por exemplo, as palavras ruído, tumulto e barulho serão substituídas ou representadas pelo termo barulho.

E, por fim, na etapa indexação atribui-se uma pontuação para cada termo, garantindo uma única instância do termo no documento.
No processo de atribuição de pesos devem ser considerados dois pontos:
\begin{itemize}
  \setlength\itemsep{0.05em}
  \item Quanto mais vezes um termo aparece no documento, mais relevante ele é para o documento;
  \item Quanto mais vezes um termo aparece na colecção de documentos, menos importante ele é para diferenciar os documentos.
\end{itemize}

\subsection{Algoritmos de \textit{machine learning}}

Os algoritmos de \textit{machine learning} são partes de código que ajudam as pessoas a explorar, analisar e localizar o significado em conjuntos de dados complexos.

Os algoritmos de \textit{machine learning} utilizam parâmetros baseados em dados de preparação, um subconjunto de dados que representa o conjunto maior.
À medida que os dados de preparação se expandem para representar o mundo de forma mais realista, o algoritmo calcula resultados mais precisos.

Algoritmos diferentes analisam os dados de diversas formas.
Geralmente, são agrupados consoante as técnicas de \textit{machine learning} para as quais são utilizados:
\begin{itemize}
  \setlength\itemsep{0.05em}
  \item Aprendizagem supervisionada;
  \item Aprendizagem não supervisionada;
  \item Aprendizagem por reforço.
\end{itemize}

\subsubsection{Aprendizagem supervisionada}

Na aprendizagem supervisionada, os algoritmos fazem previsões com base num conjunto de exemplos etiquetados fornecidos por si.
Esta técnica é útil quando sabe como deverá ser o resultado.
Por exemplo, fornece um conjunto de dados que inclui populações de cidades por ano nos últimos 100 anos e deseja saber qual será a população de uma cidade específica dentro de quatro anos.
O resultado utiliza etiquetas que já existem no conjunto de dados: população, cidade e ano.

\subsubsection{Aprendizagem não supervisionada}

Na aprendizagem não supervisionada, os pontos de dados não são etiquetados.
O algoritmo etiqueta-os ao organizar os dados ou ao descrever a sua estrutura.
Esta técnica é útil quando não sabe como deverá ser o resultado.
Por exemplo, fornece dados de cliente e deseja criar segmentos de clientes que gostam de produtos semelhantes.
Os dados que está a fornecer não são etiquetados e as etiquetas no resultado são geradas com base nas semelhanças descobertas entre os pontos de dados.

\subsubsection{Aprendizagem de reforço}

A aprendizagem por reforço utiliza algoritmos que aprendem com resultados e decide a acção a realizar em seguida.
Após cada acção, o algoritmo recebe comentários que o ajudam a determinar se a escolha feita foi correta, neutra ou incorrecta.
Por exemplo, se estivermos a criar um carro autónomo, queremos que este cumpra a lei e mantenha as pessoas seguras.
À medida que o carro ganha experiência e um histórico de reforço, aprende a permanecer dentro da faixa, a não ultrapassar o limite de velocidade e a travar quando encontrar peões.

Existem muitos tipos diferentes de algoritmos de \textit{machine learning}.Contudo, por norma, os casos de utilização destes algoritmos enquadram-se numa destas categorias.
\begin{itemize}
  \setlength\itemsep{0.05em}
  \item Algoritmos de classificação de duas classes (binários) dividem os dados em duas categorias. São úteis para perguntas com apenas duas respostas possíveis mutuamente exclusivas, incluindo perguntas de sim/não;
  \item Algoritmos de classificação multi-classe (multinomial) dividem os dados em três ou mais categorias. São úteis para perguntas com três ou mais respostas possíveis mutuamente exclusivas;
  \item Algoritmos de detecção de anomalias identificam os pontos de dados que estão fora dos parâmetros definidos para o que é considerado ``normal''
  \item Algoritmos de regressão prevêem o valor de um novo ponto de dados com base em dados históricos;
  \item Algoritmos de séries temporais mostram as alterações a um determinado valor ao longo do tempo. Com a análise e a previsão de série temporal, os dados são recolhidos a intervalos regulares ao longo do tempo e utilizados para fazer previsões e identificar tendências, sazonalidade, periodicidade e irregularidade;
  \item Algoritmos de \textit{clustering} dividem os dados por vários grupos ao determinar o nível de semelhança entre os pontos de dados;
  \item Algoritmos de classificação utilizam cálculos de previsão para atribuir dados a categorias predefinidas.
\end{itemize}

\subsection{Decisão sobre o tipo de algoritmo}

Após a grande análise dos tipos e subtipos de algoritmos de mineração de texto (análise de texto), a escolha foram os algoritmos de \textit{machine learning} com aprendizagem não supervisionada para a execução da nossa análise; a questão está em qual serão usados visto que muitos deles para os nossos casos poderão ter de ser sujeitos a pré-processamento; o que removeria as nossas requeridas dimensões de análise textual.
No entanto vai continuar a haver pré-processamento como algoritmos de redução de dimensionalidade, a diferença comparado com a frase anterior é quão o pré-processamento não afectará negativamente os resultados e possíveis associações.

\subsubsection{Algoritmos considerados}

Dos variados algoritmos vistos e disponíveis na internet ou em bibliotecas de \textit{Python} (como \textit{SciKitLearn, Tensorflow}), tomamos a decisão de considerar os seguintes algoritmos como candidatos a uso e/ou pertencentes aos grupos de algoritmos usados para comparação de resultados:
\begin{itemize}
  \setlength\itemsep{0.05em}
  \item \textit{LDA (Latent Dirichlet Allocation)}: um modelo de distribuição gaussiana, muito usado por empresas de software sobre \textit{feedback} e \textit{bug reports} para associação de resultados do \textit{QA};
  \item \textit{Naïve Bayes} \cite{mbn1}: uma família de "classificadores probabilísticos" simples baseados na aplicação do teorema de Bayes com suposições de independência fortes (ingênuas) entre os recursos;
  \item \textit{K-Means Clustering}: muito usado para fazer \textit{clusters} de \textit{keywords} em redes sociais;
  \item \textit{KNN (K-Nearest Neighbor)}: usado para agrupar dados relacionais com os contactos, relatórios, correspondência e \textit{emails} em empresas;
  \item \textit{SVM (Support Vector Machines)}: este é usado nos mesmos lugares que regressões lineares, porém mais rápido ou poderoso, é usado para agrupar pontos como texto com imagens ou tópicos de texto em sites de vendas de 2ª mão.
\end{itemize}

No entanto existe um algoritmo de \textit{machine learning} semi-supervisto (aprendizagem não supervista mas ele faz a sua auto-supervisão; logo é questão de semântica).
Este é o:
\begin{itemize}
  \setlength\itemsep{0.05em}
  \item LSTM (\textit{Long-short term memory}): que é um tipo de RNN (rede neural recorrente) com ou sem um \textit{layer} CNN (um \textit{layer} de convulsão).
\end{itemize}

Pode-se notar que aqui foram escolhidos algoritmos de \textit{machine learning} já comuns a grupos que realizaram tarefas semelhantes e que são algoritmos simples e rápido não sendo mais um algoritmo de uma família de algoritmos (mais complexos ou não, mas que têm muita variedade), tais como redes neurais (à exceção do RNN LSTM), algoritmos genéticos ou algoritmos lineares (como regressões lineares).

Porém no decorrer do trabalho o algoritmo LSTM não funcionou da melhor maneira, e optámos por outras técnicas que serão explicadas, justificadas e demonstradas mais adiante no relatório.

\newpage
\section{Resumo dos Passos de Execução}

A seguinte tabela descreve os passos de actuação.

\begin{center}
    \fbox{\includegraphics[scale=1]{flowchart/flowchart.pdf}}
    \label{table:1}
\end{center}