\chapter{Análise de Dados (\textit{Data Mining})}
\label{cap4}

\section{Pré-processamento}

Esta etapa consiste em remover todos os caracteres especiais, que não são letras, números ou espaços \cite{u1}. As quais podemos considerar diacrítico como caracteres especiais ou não. Dependendo do caso, podemos remover todos os caracteres especiais, ou apenas os que não são diacríticos \cite{cr1}.

Nos nossos casos de análise textual, houve necessidade de remover todos os caracteres especiais, mas os diacríticos podem ser úteis para a nossa análise, em especial nos passos seguintes, extracção de \textit{keywords} e análise de sentimentos, foram descartados ou usados respectivamente, pelo motivo da precisão da análise em questão.

\subsection{Metodologia}

Para remover os caracteres especiais, foram criados dois \textit{scripts}, um para limpar os ficheiros \textit{.csv} e outro tratar do texto em si, que consiste em remover os caracteres especiais, fazer a normalização dos caracteres via \textit{NFKD}\cite{nfkd1}, isto é decompor os caracteres e recompor apenas pela forma canónica equivalente, e aplicar a remoção de acentos \cite{cr1}, caso seja necessário (apenas alterando o último passo de \textit{encoding}/\textit{decoding}).

Estes não requerem bibliotecas externas, podem ser executados em \textit{Python 3}, e as suas bibliotecas \textit{standard}.

\subsubsection{Limpar ficheiros}

O \textit{script trimer.py} é responsável fazer \textit{trimming} (remover espaços em branco), remover linhas em branco e colocar aspas duplas em cada \textit{review}. Para estes foi necessário o uso de expressões regulares \cite{u1}.

\subsubsection{Limpar texto dos \textit{reviews}}

O \textit{script normalize} é responsável por remover caracteres especiais, fazer a normalização dos caracteres via \textit{NFKD} \cite{nfkd1} e aplicar a remoção de acentos \cite{cr1}, caso seja necessário (apenas alterando o último passo de \textit{encoding}/\textit{decoding}).

\subsection{Execução}

Executamos o \textit{script trimer.py} para limpar os ficheiros \textit{.csv} \cite{gfg1} e o \textit{script} \textit{normalize} para limpar o texto dos \textit{reviews}. Estes aceitam um caminho para uma pasta com os ficheiros \textit{.csv} e itera sobre os ficheiros, executando as funções de limpeza.

Após a execução da limpeza textual e normalização sem remoção de acentos, seguidamente realizámos com remoção de acentos para que tenhamos ambas as versões. Para a versão com remoção de acentos, foi necessário usar o pacote \textit{Unidecode} para aplicar a remoção de acentos \cite{cr1}.

Com estas duas versões podemos obter os resultados óptimos para a nossa análise textual.

\subsection{Resultados}

Os ficheiros \textit{.csv} foram limpos e normalizados sem remoção de acentos e com remoção de acentos.

\section{Extracção de \textit{keywords}}

A extracção de \textit{keywords} é uma técnica de extracção de informações que consiste em extrair \textit{keywords} de um texto.

Geralmente, as \textit{keywords} são utilizadas para identificar o conteúdo de um documento \cite{tamyt1}, ou seja, para identificar o que o documento contém \cite{tamyt2}. No nosso caso, as \textit{keywords} são utilizadas para identificar pontos fulcrais da recepção de um cliente turístico num estabelecimento turístico de Beja (hotel, atracção, restaurante, etc).

Com estas \textit{keywords}, é possível identificar o que o cliente quer, ou seja, o que ele quer ver, o que ele quer comer, o que ele quer fazer, ou o nível de satisfação com o serviço prestado.

\subsection{Metodologia}

Para a extracção de \textit{keywords}, utilizamos uma biblioteca de \textit{Python} chamada \href{http://yake.inesctec.pt/}{\textit{YAKE}}. Esta é um \textit{pipeline} de processamento de linguagem natural, que utiliza um algoritmo personalizado descendente do \textit{Naïve Bayes} para extrair \textit{keywords} de um texto. 

O \href{http://yake.inesctec.pt/}{\textit{YAKE}} é um \textit{software} livre, e pode ser obtida via \textit{pip} ou via \textit{GitHub}, que é uma ferramenta de extracção de \textit{keywords} desenvolvida por autores portugueses (e um japonês) da Universidade do Porto, Politécnico de Tomar, e da Universidade da Beira Interior (e da Universidade de Kyoto).
A sua utilização pode ser simples, basta instalar a biblioteca e executar o seguinte comando: \texttt{yake.KeywordExtractor(lan="pt").extract\textunderscore keywords(text)}

Para a extracção de \textit{keywords} de cada \textit{review}, utilizamos o \textit{YAKE} iterativamente por cada \textit{review}, alimentando-o com o seu conteúdo textual, após o pré-processamento textual de todas as \textit{reviews} (a qual especificamente foi usada a versão sem diacríticos).

O \textit{output} da alimentação do \textit{YAKE} é um par de \textit{keywords} e seus respectivos pesos, que são armazenados num dicionário \cite{tamgh1}. O dicionário é ordenado pelo valor de seus pesos \cite{tamyt3}, e o número de \textit{keywords} extraídas é limitado ao número de \textit{reviews} que foram utilizadas para a extracção por cada estabelecimento.

No entanto esta pode ser mais complexa caso seja necessário optimizar a extracção de \textit{keywords}, com os seus variados parâmetros opcionais.

Este funciona da seguinte forma: quando recebe um texto, vai testar todas as palavras do texto, com uma determinada fórmula, e guardar o peso da palavra, e a palavra em si, no final expele um dicionário com as \textit{keywords} e seus respectivos pesos.

A fórmula falada anteriormente é:
\begin{math}
    S(kw) = \frac{\prod_{w \in kw}^{} S(kw)}{TF(kw) * \sum_{w \in kw}^{}S(w)}
\end{math}

Mais especificamente este módulo é uma forma mais delicada e avançada de um classificador \textit{Naïve Bayes} \cite{tamgh1} \cite{tamyt1} \cite{tamyt2} \cite{tamyt3} o qual será mais e melhor explicado no capítulo seguinte onde procedemos ao desenvolvimento de um para efeitos de análise de sentimentos.

\subsubsection{Execução}

Foi criado um \textit{notebook} de \textit{Jupyter} o qual contém o código usado para a extracção de palavras via \textit{YAKE}. Cada bloco de código está sobreposto por um bloco de \textit{markdown}, que é um comentário. Os comentários são usados para explicar o que cada bloco de código faz.

As rotinas de extracção de \textit{keywords} são: iterativamente, para cada ficheiro \textit{.csv} dentro da pasta indicada, importar via \textit{DataFrame} de \textit{pandas}. O qual \textit{DataFrame} é um conjunto de dados, como uma tabela de dados, e contém uma coluna com os \textit{reviews}. O \textit{DataFrame} é iterado na sua coluna única, e o seu conteúdo é passado para o \textit{YAKE}. O qual exporta o resultado para um ficheiro \textit{.csv}, que é um ficheiro \textit{.csv} com uma coluna com as \textit{keywords} e outra com o seu peso.

O \textit{notebook} e o código estão disponíveis nos apêndices \autoref{ap9}

\subsection{Resultados}

Os resultados obtidos detêm um sentimento misto no grupo. Estas \textit{keywords} por vezes não são palavras únicas, mas são expressões que são frequentes. Muitas palavras únicas aparecem lado a lado das expressões, o que dá uma noção de repetição ou confirmação de resultado.

\section{Análise de Sentimentos}

A análise de sentimentos é uma técnica de extracção de informações que consiste em extrair sentimentos de um texto.

Geralmente, os sentimentos são utilizados para identificar o sentimento de um texto, ou seja, para identificar o que o texto contém. No nosso caso, os sentimentos são utilizados para identificar a satisfação do cliente com o serviço prestado em variados serviços turísticos de Beja (hotéis, atracções, restaurantes, etc).

Com base no texto, o sentimento é extraído através de um algoritmo que identifica a intensidade do sentimento. No nosso caso, a classificação é binária, ou seja, o sentimento é positivo ou negativo. Consideramos uma não-reclamação como positiva.

A percentagem de sentimentos positivos e negativos é necessária para identificar a satisfação do cliente com o serviço prestado. A satisfação do cliente é uma medida de qualidade de serviço.

\subsection{Metodologia}

Para fazer uma análise de sentimento textual, utilizámos um modelo de \textit{machine learning} chamado \textit{BERT} baseado em \textit{Transformers}, a qual criámos um modelo de classificação de texto binário ou ternário, no nosso caso, o modelo é um classificador de sentimentos binário.

Este modelo é treinado através da alimentação de um conjunto de dados de treino e um conjunto de dados de teste, ao modelo matemático criado ou importado. Estes modelos matemáticos podem ser probabilísticos ou não. Os dados de treino são utilizados para treinar o modelo matemático. Os dados de teste são utilizados para testar o modelo matemático.

Após o treino do modelo matemático, e os resultados do teste (matriz de confusão, precisão, exactidão, etc), o modelo é utilizado para classificar um texto. O resultado da classificação é o sentimento do texto, ou seja, positivo ou negativo.

Este trabalho de treino, teste e validação, no caso do modelo final escolhido, foi feito pelas empresas/comunidades responsáveis e patrocinadoras do projecto. Porém nas varias tentativas de desenvolvimento, com outros modelos, também fizemos um conjunto destas tarefas.

Para pré-processar o texto a ser classificado (e os textos de treino e teste), utilizámos o pacote de linguagem natural do \textit{Python}, ou seja, a biblioteca \textit{NLTK}, para aplicar algumas transformações ao texto, como remover pontuação, remover \textit{stopwords}, etc.

Este passo é o mais importante para o modelo matemático ser bem treinado e o mais importante para o modelo ser bem classificado.

\subsubsection{Pré-processamento}

Para fazer o pré-processamento do texto, utilizámos as ferramentas do \textit{NLTK}, das quais em especial os que fazem (ou determinam) as \textit{stopwords}, fazem \textit{stemming}, \textit{tokenização}, etc.

Mais especificamente \textit{stopwords} portuguesas do \textit{corpus} de \textit{stopwords} do \textit{NLTK}, o \textit{SnowballStemmer}, é utilizado para remover \textit{stopwords} e os sufixos das palavras deixando apenas a raiz da palavra (\textit{the stem}), e o \textit{Vectorizer} que é utilizado para transformar o texto em um vector (ou matriz) de características (termos numéricos).

Este último é preferencial que se use o \textit{CountVectorizer}, pois ele conta o número de vezes que um termo aparece no texto com inteiros, e não com \textit{floats}, como o \textit{TF-IDF} faz. Sendo o último mais preferencial para outras tarefas (não classificação).

O \textit{Stemming} é para nós a fase mais importante do processo de pré-processamento, pois é o que removemos os sufixos das palavras.
Para muitos propósitos, e em especial este, a conjugação dos verbos atrapalha a classificação e a aprendizagem do modelo (ou até a nossa). Um exemplo da sua desnecessidade é a enorme diferença entre o Inglês e o Português.

Os verbos em inglês são conjugados com substantivos, e os substantivos são conjugados com verbos. Porém, os verbos em português são conjugados com adjectivos, e os adjectivos são conjugados com verbos. Mais a enorme variação entre pessoas e tempos, em inglês variamos de três formas para seis (nove se separarmos \textit{he/she/it}) pessoas para três tempos (um presente e dois passados, futuros e condicionais são modificações de um presente), já em português variamos de seis formas para seis pessoas (oito se separarmos ele/ela e eles/elas) para seis tempos (um presente, três passados, um futuro e um condicional).

Estas conjugações tem a mesma acção e uma enorme semelhança frásica: a raiz do verbo (\textit{the stem}). As conjugações são desnecessárias e demasiado complexas.

Sendo assim, ao aplicar \textit{stemming}, reduzimos a dimensão da matriz/vector, e simplificamos a linguagem, de maneira inteligente.

\subsection{Tentativas}

Foram feitas três tentativas de classificação de sentimentos. A primeira foi um fracasso completo, foi tentada a criação de um modelo sequencial com \textit{layers LSTM} com \textit{embedding} e de convulsão. À falta de conhecimento prévio, e à falta de informações simples e palpáveis com acesso fácil na \textit{internet}, não foi possível fazer a classificação de sentimentos com este modelo. Este modelo foi tentado com o uso da biblioteca de \textit{TensorFlow}, que é uma biblioteca de código aberto.

A segunda tentativa foi muito mais bem sucedida que a primeira. Foi usado para o algoritmo de classificação de sentimentos o \textit{Naïve Bayes} (\textit{Multinomial}), e como dados de treino e teste, foram utilizados os dados de treino e teste disponíveis no \textit{Kaggle}, estes usavam \textit{reviews} de filmes e produtos de \textit{e-commerce} em português do \textit{Brasil}, já que em português de Portugal não foi possível encontrar.

\subsubsection{Modelo Sequencial \textit{LSTM} com \textit{Embedding}}

Este modelo não chegou a completar qualquer fase de treinamento, pois não foi possível criar um modelo que funcionasse com o \textit{dataset} de treino, a quantidade absurda de variáveis e modelações junto com a falta de conhecimento prévio impossibilitaram que um modelo funcional fosse criado, inclusive com \textit{trimming} ou \textit{truncation} dos \textit{inputs}.

\paragraph{Execução\\}

Após a importação do pacote de \textit{TensorFlow}, foi criado um modelo sequencial com o uso de \textit{embeddings}, ou seja, um modelo que utiliza \textit{embeddings} para representar os \textit{inputs}. Os \textit{layers LSTM} e \textit{Dropout} foram utilizados para aumentar a capacidade de aprendizagem do modelo. E os \textit{layers} de saída são os \textit{softmax} e \textit{categorical\textunderscore crossentropy}.

Cada um destes \textit{layers} detinha parâmetros que teriam de ser ajustados para que o modelo fosse capaz de classificar os sentimentos. O qual requeria algum apoio não disponível, ou seja, o modelo não foi capaz de aprender a classificar sentimentos.

\paragraph{Resultados\\}

Não foi possível executar a tentativa de classificação de sentimentos com este modelo.

\subsubsection{Modelo \textit{Naïve Bayes} do \textit{Scikit-Learn} com \textit{Datasets PT-BR} do \textit{Kaggle}}

Nesta tentativa de classificação de sentimentos, foi utilizado o pacote \textit{Scikit-Learn}, e foi utilizado um par de \textit{datasets} de \textit{reviews} do \textit{Kaggle}, que é um \textit{dataset} de \textit{reviews} de filmes e produtos de \textit{e-commerce} em português do \textit{Brasil}, já que em português de Portugal não foi possível encontrar.

Foi criado um objecto (que deriva da nossa classe \textit{StemmerTokenizer}), que foi utilizado para aplicar o \textit{Stemming} e \textit{Tokenização} aos dados de treino e teste. Reduziu-se a quantidade de dados, e aumentou a capacidade de aprendizagem do modelo. A necessidade de aplicar \textit{stemming} à \textit{Tokenização} veio da necessidade de reduzir a dimensão da matriz/vector, e simplificar a linguagem, de maneira inteligente.

Sendo assim, ao aplicar \textit{stemming}, reduzimos a dimensão da matriz/vector, e simplificamos a linguagem, de maneira inteligente.

\paragraph{\textit{Naïve Bayes}\\}

Um classificador \textit{Naïve Bayes} é um classificador simples e probabilístico baseado em aplicar a teoria de \textit{Bayes} com assumpções fortes ({naïve}) de independência \cite{mbn1}. \textit{Naïve Bayes classifiers} são simples e fáceis de entender, requerendo nenhuma etapa de \textit{prunning} para evitar o \textit{overfitting}. Contudo, eles não são mais poderosos do que outras técnicas avançadas, como árvores de decisão ou vector de suporte, muito menos que máquinas neurónios.

\paragraph{Como funciona um classificador \textit{Naïve Bayes}?\\}

Estes classificadores são baseados em aplicar a teoria de \textit{Bayes} com assumpções fortes (\textit{naïve}) de independência \cite{mbn1}. A hipótese \textit{naïve} diz que os \textit{features} são independentes uns dos outros. Isso significa que os \textit{features} são condicionais independentes a partir da classe \cite{gfg2}. Indústria matemática pode ser usada para mostrar que os \textit{features} são independentes a partir da classe, portanto a classe é a mais provável de saída.

A ideia principal do \textit{Naïve Bayes} é calcular a probabilidade de cada classe, dado os \textit{features}. A probabilidade de uma classe é calculada por multiplicar as probabilidades de cada \textit{feature} dado a classe \cite{mbn1}.

Este é um exemplo simples de um classificador \textit{Naïve Bayes}: recebe um vector de \textit{features} e um rótulo de classe e retorna a probabilidade da classe dado os \textit{features} \cite{skl1}. No problema de classificação de texto, o rótulo de classe é a classe real do texto. Se inserirmos um vector de \textit{features} do texto e o rótulo de classe, o classificador \textit{Naïve Bayes} retorna a probabilidade da classe dado os \textit{features} \cite{gfg2}.

Outros exemplos de classificadores \textit{Naïve Bayes} são: análise de sentimentos, detecção de \textit{spam}, e classificação de texto \cite{mbn1}. Como para nosso caso (o problema de análise de sentimentos), temos um vector de \textit{features} do texto e o rótulo de classe é o sentimento do texto. O conjunto de classe é um conjunto binário de positivo e negativo \cite{gfg2}. Recebe um vector de \textit{features} \cite{skl1} do texto e o rótulo de classe e retorna a probabilidade de zero a um de classe, sendo o mais próximo a um o mais positivo o texto é.

\paragraph{Execução\\}

Foi criado um \textit{notebook} para a execução da tarefa. Este \textit{notebook}, contem o código do modelo \textit{Naïve Bayes} do \textit{Scikit-Learn} \cite{skl1}, e as referencias para \textit{datasets} de treino e teste. Com blocos alternados de código e \textit{markdown} que comentam a execução do modelo, foi possível executar a tentativa de classificação de sentimentos com este modelo.

As rotinas executadas foram: a criação de um objecto \textit{StemmerTokenizer} \cite{skl1}, a criação de um modelo \textit{Naïve Bayes} do \textit{Scikit-Learn} \cite{skl1}, o treinamento do modelo, em que de fazia \textit{drop} a inúmeras colunas, os testes e métricas de avaliação do modelo, a importação iterativa dos \textit{.csv} que contém os \textit{reviews} dos estabelecimentos em que iterativamente foi aplicada uma limpeza e normalização, e a execução do modelo nos \textit{reviews}, que quando classificados, geraram um arquivo \textit{.csv} com os resultados.

\paragraph{Resultados\\}

Este modelo foi capaz de classificar sentimentos com sucesso. A classificação foi bem sucedida, e foi possível classificar os sentimentos de um conjunto de \textit{reviews}, e gerar um arquivo \textit{.csv} com os resultados. Porém a classificação foi pouco precisa, e a precisão real foi baixa, apesar de ser um modelo bem sucedido e de ter sido bem treinado e avaliado. A sua \textit{accuracy} foi de entre 91\% e 87\%, dependendo dos \textit{runs}.

Calculámos que o problema está na diferença entre o português de Portugal e o português de \textit{Brasil}, o qual o último é a língua dos \textit{datasets} de treino e teste, o qual o modelo foi bastante preciso; e o primeiro a língua materna dos estabelecimentos e dos clientes, o qual o modelo foi pouco preciso.

\subsubsection{\textit{Pipelines} de \textit{Transformers} do \textit{HuggingFace} e Modelos \textit{BERT} da Google \textit{fine-tuned}}

Como os resultados finais do modelo \textit{Naïve Bayes} não foram satisfatórios, foi criado um \textit{pipeline} de \textit{transformers} do \textit{HuggingFace} \cite{yt2}, com um modelo \textit{BERT} da \textit{Google} \textit{fine-tuned} para classificar sentimentos de forma binária.

Esta biblioteca pode usar \textit{PyTorch} ou \textit{TensorFlow} como \textit{backend} de computação, cada um destes tem suas vantagens e desvantagens. Por defeito ele usa \textit{PyTorch} na maioria dos seus \textit{pipelines}, mas pode ser configurado para usar \textit{TensorFlow}.

Estes \textit{pipelines} do \textit{HuggingFace} são muito mais complexos, são muito mais eficientes, e também são muito flexíveis e fáceis de usar\cite{yt2}. São definidos com um texto que determina qual é o \textit{pipeline}, e os parâmetros pedidos. No nosso caso foi utilizado o \textit{pipeline} de classificação de sentimentos em que tinha dois parâmetros, o \textit{tokenizer} e o modelo, mais especificamente um \textit{AutoModelForSequenceClassification} e um \textit{AutoTokenizer}, os quais foram buscar o modelo \textit{pretrained} e o \textit{tokenizer} do \href{https://huggingface.co/gchhablani/bert-base-cased-finetuned-sst2}{\texttt{gchhablani/bert-base-cased-finetuned-sst2}} \cite{yt2}.

\paragraph{O que é um Transformer\\}

Um \textit{Transformer} é um tipo de rede neural que é capaz de aprender funções complexas de dados. Ele funciona através de transformar os dados de entrada em uma nova representação, que pode então ser usada para fazer predições em novos dados.

\textit{Transformers} tem muitas aplicações em linguagens de processamento natural, processamento de imagens e visão computacional. Eles foram bem-sucedidos em diversos domínios, incluindo:
\begin{itemize}
  \setlength\itemsep{0.05em}
    \item Sumários de texto (\textit{BERT, DistilBert, RoBERTa, XLNet})
    \item Captação de imagens (\textit{XLNet})
    \item Tradução de imagens (\textit{BERT, DistilBert, RoBERTa})
    \item Respostas a perguntas (\textit{BERT, DistilBert, RoBERTa})
    \item \textit{Chatbots} (\textit{BERT, DistilBert, RoBERTa})
    \item Classificação (\textit{BERT, DistilBert, RoBERTa})
    \item \textit{Entre outros}.
\end{itemize}

O primeiro \textit{Transformer} introduzido foi o \href{https://arxiv.org/abs/1810.04805}{BERT model}. Foi desenvolvido pela \href{https://www.google.com/}{Google}. É um modelo de sequência-para-sequência que tem um \textit{encoder} e um \textit{decoder}. O \textit{encoder} mapeia uma sequência de \textit{tokens} de entrada para uma sequência de estados ocultos. O \textit{decoder} toma o output do \textit{encoder} e tenta mapeá-lo de volta para a sequência original de \textit{tokens} \cite{yt2}.

A \textit{NVIDIA} e a \textit{Facebook} desenvolveram \href{https://arxiv.org/abs/1906.08237}{\textit{XLNet}} e \href{https://arxiv.org/abs/1810.04805}{\textit{DistilBert}} \cite{yt2}. São semelhantes ao \textit{BERT}, mas tem uma arquitectura diferente e um conjunto de pesos diferentes. \textit{XLNet} é um modelo de sequência-para-sequência que tem um \textit{encoder} e um \textit{decoder}. O \textit{encoder} mapeia uma sequência de \textit{tokens} de entrada para uma sequência de estados ocultos. O \textit{decoder} recebe o output do \textit{encoder} e tenta mapeá-lo de volta para a sequência original de \textit{tokens}. \textit{DistilBert} é similar ao \textit{XLNet}, mas é treinado em um subconjunto muito menor do que o data.

\paragraph{Como funciona um \textit{Transformer}?\\}

A funcionalidade de um \textit{Transformer} é transformar uma sequência de \textit{tokens} de entrada em uma sequência de \textit{tokens} de saída. Os \textit{tokens} \cite{mtf1} \cite{mtf2} de entrada são geralmente palavras, mas eles também podem ser outros tipos de \textit{tokens}, como imagens de captação. Os \textit{tokens} de saída são geralmente iguais aos \textit{tokens} de entrada, mas eles podem ser diferentes \cite{hf1}.

Ele funciona através de transformar os \textit{tokens} de entrada em uma nova representação, que pode então ser usada para fazer predições em novos dados. Os \textit{layers} de sequência são chamados de \textit{encoder} \cite{mtf1} e de sequência. Elas têm x \textit{layers} de \textit{encoder} e y \textit{layers} de \textit{decoder}. x e y são geralmente iguais, mas eles podem ser diferentes. Os \textit{layers} podem ser diferentes tamanhos \cite{hf1}.

Especificamente o \textit{Transformer} é um modelo de sequência-para-sequência. Ele usa \textit{Embeddings} para os \textit{tokens} de entrada e posiciona-os antes de serem enviados para o primeiro conjunto de \textit{layers}. O primeiro conjunto de \textit{layers} tem um \textit{self-attention} \cite{mtf2} mecanismo que toma os \textit{tokens} \cite{mtf1} de entrada e transforma-os em uma nova representação que é adicionada e normalizada antes de ser enviada para o segundo conjunto de \textit{layers} onde os primeiros passos são iguais aos anteriores mas eles usam os \textit{outputs} anteriores como \textit{inputs}. Essa nova \textit{layer} combina os inputs com os \textit{outputs} antigos e os \textit{outputs} são enviados para o próximo conjunto de \textit{layers}. Isso é repetido até que o \textit{output} seja o mesmo que o \textit{input}. Então ele passa por um \textit{layer} linear \textit{regression} e um \textit{softmax}. O \textit{output} é o final \textit{output \cite{hf1}}.

\paragraph{\textit{Layers} em específico\\}

Os três mais importantes tipos de \textit{layers} \cite{hf1} são:
\begin{itemize}
  \setlength\itemsep{0.05em}
    \item A \textit{softmax function} é uma função comum em redes \textit{neurais}. Ela toma um vector e retorna um vector com o mesmo tamanho. A \textit{softmax function} é usada para normalizar o \textit{output} da rede. Ela é usada para garantir que o \textit{output} é uma distribuição probabilística \cite{mtf1}.
    \item Uma regressão linear é uma função que toma um vector e retorna um vector. Ela é usada para fazer predições usando um meio probabilístico simples e comum.
    \item A função de \textit{self-attention} é feita usando uma combinação de regressão linear e a \textit{softmax}, com ou sem paralelismo (\textit{multi-head attention}), a função de \textit{self-attention} é usada para imprimir uma importância para o conjunto de palavras que está sendo avaliado.
\end{itemize}

\paragraph{Execução\\}

No nosso \textit{notebook}, foi criado um \textit{pipeline} de \textit{transformers} do \textit{HuggingFace}, com um modelo \textit{BERT} da \textit{Google} \textit{fine-tuned}\cite{hf1} para classificar sentimentos de forma binária. E foi criado um \textit{notebook} para a execução da tarefa. Este \textit{notebook}, contém o código do pipeline de \textit{transformers} do \textit{HuggingFace}, e as referências para o modelo \textit{pretrained} e o \textit{tokenizer} desse modelo.

Com blocos alternados de código e \textit{markdown} que comentam a execução do pipeline, foi possível executar a classificação de sentimentos com este pipeline. Iterativamente fomos buscar os \textit{.csv} dos \textit{reviews} dos \textit{establecimentos}, e foi aplicado o \textit{pipeline} de \textit{transformers} do \textit{HuggingFace} sequencialmente a cada \textit{review}, e ao final, foi gerado um arquivo \textit{.csv} com os resultados do local.

Este \textit{pipeline} foi o mais rápido das três tentativas de classificação de sentimentos, e foi o mais preciso das três tentativas de classificação de sentimentos.

\paragraph{Resultados\\}

Os resultados foram bem sucedidos, e foi possível classificar os sentimentos de um conjunto de \textit{reviews}, e gerar um arquivo \textit{.csv} com os resultados. A classificação foi bem sucedida e bastante precisa, e foi possível classificar os sentimentos de um conjunto de \textit{reviews}, e gerar um arquivo \textit{.csv} com os resultados.

De acordo com a documentação deste modelo (e do \textit{BERT} original da \textit{Google}), a classificação foi bem precisa, e a precisão foi de entre 97\% e 98\%. Dependendo da tarefa o \textit{BERT} pode até chegar a quase 100\% de precisão, neste caso desce devido à natureza binária da classificação.

Embora não sejam 100\% precisos, o \textit{BERT} foi o mais preciso das três tentativas de classificação de sentimentos e os resultados foram bastante satisfatórios.

\subsection{Ponderação dos Resultados finais}

Os resultados obtidos podem não ser 100\% precisos, mas ainda assim, podem ser bastante satisfatórios. O qual podemos considerar como um resultado final, e que pode ser usado como um indicador de qualidade do estabelecimento em avaliação.