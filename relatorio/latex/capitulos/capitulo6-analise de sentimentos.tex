\chapter{Análise de sentimentos}
\label{cap6}

\section{Analise de Sentimentos}

A analise de sentimentos é uma técnica de extracção de informações que consiste em extrair sentimentos de um texto.

Geralmente, os sentimentos são utilizados para identificar o sentimento de um texto, ou seja, para identificar o que o texto contém. No nosso caso, os sentimentos são utilizados para identificar a satisfação do cliente com o serviço prestado em variados serviços turísticos de Beja (hotéis, atracções, restaurantes, etc).

Com base no texto, o sentimento é extraído através de um algoritmo que identifica a intensidade do sentimento. No nosso caso, a classificação é binária, ou seja, o sentimento é positivo ou negativo. Consideramos uma não-reclamação como positiva.

A percentagem de sentimentos positivos e negativos é necessária para identificar a satisfação do cliente com o serviço prestado. A satisfação do cliente é uma medida de qualidade de serviço.

\subsection{Metodologia}

Para fazer uma analise de sentimento textual, utilizamos \textit{machine learning}, a qual criamos um modelo de classificação de texto binário ou ternário, no nosso caso, o modelo é um classificador de sentimentos binário.

Este modelo é treinado através da alimentação de um conjunto de dados de treinamento e um conjunto de dados de teste, ao modelo matemático criado ou importado. Estes modelos matemáticos podem ser probabilísticos ou não. Os dados de treinamento são utilizados para treinar o modelo matemático. Os dados de teste são utilizados para testar o modelo matemático.

Após o treinamento do modelo matemático, e os resultados do teste (matriz de confusão, precisão, exactidão, etc), o modelo é utilizado para classificar um texto. O resultado da classificação é o sentimento do texto, ou seja, positivo ou negativo.

Para pre-processar o texto a ser classificado (e os textos de treinamento e teste), utilizamos o pacote de linguagem natural do \textit{Python}, ou seja, a biblioteca NLTK, para aplicar algumas transformações ao texto, como remover pontuação, remover \textit{stopwords}, etc.

Este passo é o mais importante para o modelo matemático ser bem treinado e o mais importante para o modelo ser bem classificado.

\subsubsection{Pre-processamento}

Para fazer o pre-processamento do texto, utilizamos as ferramentas do NLTK, das quais em especial os que fazem (ou determinam) as \textit{stopwords}, fazem \textit{stemming}, \textit{tokenização}, etc.

Mais especificamente \textit{stopwords} portuguesas do \textit{corpus} de \textit{stopwords} do NLTK, o \textit{SnowballStemmer}, é utilizado para remover \textit{stopwords} e os sufixos das palavras deixando apenas a raiz da palavra (\textit{the stem}), e o \textit{Vectorizer} que é utilizado para transformar o texto em um vector (ou matriz) de características (termos numéricos).

Este ultimo é preferencial que se use o \textit{CountVectorizer}, pois ele conta o número de vezes que um termo aparece no texto com inteiros, e não com \textit{floats}, como o \textit{TF-IDF} faz. Sendo o ultimo mais preferencial para outras tarefas (não classificação).

O \textit{Stemming} é para nós a fase mais importante do processo de pre-processamento, pois é o que removemos os sufixos das palavras.
Para muitos propósitos, e em especial este, a conjugação dos verbos atrapalha a classificação e a aprendizagem do modelo (ou até a nossa). Um exemplo da sua \textit{desnecessidade} é a enorme diferença entre o Inglês e o Português.

Os verbos em inglês são conjugados com substantivos, e os substantivos são conjugados com verbos. Porém, os verbos em português são conjugados com adjectivos, e os adjectivos são conjugados com verbos. Mais a enorme variação entre pessoas e tempos, em inglês variamos de três formas para seis (nove se separarmos \textit{he/she/it}) pessoas para três tempos (um presente e dois passados, futuros e condicionais são modificações de um presente), já em português variamos de seis formas para seis pessoas (oito se separarmos ele/ela e eles/elas) para seis tempos (um presente, três passados, um futuro e um condicional).

Estas conjugações tem a mesma acção e uma enorme semelhança frásica: a raiz do verbo (\textit{the stem}). As conjugações são desnecessárias e demasiado complexas.

Sendo assim, ao aplicar \textit{stemming}, reduzimos a dimensão da matriz/vector, e simplificamos a linguagem, de maneira inteligente.

\subsection{Tentativas}

Foram feitas três tentativas de classificação de sentimentos. A primeira foi um fracasso completo, foi tentada a criação de um modelo sequencial com \textit{layers} LSTM com \textit{embedding} e de convulsão. À falta de conhecimento prévio, e à falta de informações simples e palpáveis com acesso fácil na \textit{internet}, não foi possível fazer a classificação de sentimentos com este modelo. Este modelo foi tentado com o uso da biblioteca de \textit{TensorFlow}, que é uma biblioteca de código aberto.

A segunda tentativa foi muito mais bem sucedida que a primeira. Foi usado para o algoritmo de classificação de sentimentos o \textit{Naïve Bayes} (\textit{Multinomial}), e como dados de treino e teste, foram utilizados os dados de treino e teste disponíveis no \textit{Kaggle}, estes usavam \textit{reviews} de filmes e produtos de \textit{e-commerce} em português do \textit{Brasil}, já que em português de Portugal não foi possível encontrar.

\subsubsection{Modelo Sequencial LSTM com \textit{Embedding}}

Este modelo não chegou a completar qualquer fase de treinamento, pois não foi possível criar um modelo que funcionasse com o \textit{dataset} de treino, a quantidade absurda de variáveis e modelações junto com a falta de conhecimento prévio impossibilitaram que um modelo funcional fosse criado, inclusive com \textit{trimming} ou \textit{truncation} dos \textit{inputs}.

\paragraph{Execução\\}

Após a importação do pacote de \textit{TensorFlow}, foi criado um modelo sequencial com o uso de \textit{embeddings}, ou seja, um modelo que utiliza \textit{embeddings} para representar os inputs. Os \textit{layers} LSTM e \textit{Dropout} foram utilizados para aumentar a capacidade de aprendizagem do modelo. E os \textit{layers} de saída são os \textit{softmax} e \textit{categorical\textunderscore crossentropy}.

Cada um destes \textit{layers} detinha parâmetros que teriam de ser ajustados para que o modelo fosse capaz de classificar os sentimentos. O qual requeria algum apoio não disponível, ou seja, o modelo não foi capaz de aprender a classificar sentimentos.

\paragraph{Resultados\\}

Não foi possível executar a tentativa de classificação de sentimentos com este modelo.

\subsubsection{Modelo \textit{Naïve Bayes} do \textit{Scikit-Learn} com \textit{Datasets PT-BR} do \textit{Kaggle}}

Nesta tentativa de classificação de sentimentos, foi utilizado o pacote \textit{Scikit-Learn}, e foi utilizado um par de \textit{datasets} de \textit{reviews} do \textit{Kaggle}, que é um \textit{dataset} de \textit{reviews} de filmes e produtos de \textit{e-commerce} em português do \textit{Brasil}, já que em português de Portugal não foi possível encontrar.

Foi criado um objecto (que deriva da nossa classe \textit{StemmerTokenizer}), que foi utilizado para aplicar o \textit{Stemming} e \textit{Tokenização} aos dados de treino e teste. Reduziu-se a quantidade de dados, e aumentou a capacidade de aprendizagem do modelo. A necessidade de aplicar \textit{stemming} à \textit{Tokenização} veio da necessidade de reduzir a dimensão da matriz/vector, e simplificar a linguagem, de maneira inteligente.

Sendo assim, ao aplicar \textit{stemming}, reduzimos a dimensão da matriz/vector, e simplificamos a linguagem, de maneira inteligente.

\paragraph{\textit{Naïve Bayes}}

Um classificador \textit{Naïve Bayes} é um classificador simples e probabilístico baseado em aplicar a teoria de \textit{Bayes} com assumpções fortes ({naïve}) de independência. \textit{Naïve Bayes classifiers} são simples e fáceis de entender, requerendo nenhuma etapa de \textit{prunning} para evitar o \textit{overfitting}. Contudo, eles não são mais poderosos do que outras técnicas avançadas, como árvores de decisão ou vector de suporte, muito menos que máquinas neurónios.

\paragraph{Como funciona um classificador \textit{Naïve Bayes}?}

Estes classificadores são baseados em aplicar a teoria de \textit{Bayes} com assumpções fortes (\textit{naïve}) de independência. A hipótese \textit{naïve} diz que os \textit{features} são independentes uns dos outros. Isso significa que os \textit{features} são condicionais independentes a partir da classe. Indústria matemática pode ser usada para mostrar que os \textit{features} são independentes a partir da classe, portanto a classe é a mais provável de saída.

A ideia principal do \textit{Naïve Bayes} é calcular a probabilidade de cada classe, dado os \textit{features}. A probabilidade de uma classe é calculada por multiplicar as probabilidades de cada \textit{feature} dado a classe.

Este é um exemplo simples de um classificador \textit{Naïve Bayes}: recebe um vector de \textit{features} e um rótulo de classe e retorna a probabilidade da classe dado os \textit{features}. No problema de classificação de texto, o rótulo de classe é a classe real do texto. Se inserirmos um vector de \textit{features} do texto e o rótulo de classe, o classificador \textit{Naïve Bayes} retorna a probabilidade da classe dado os \textit{features}.

Outros exemplos de classificadores \textit{Naïve Bayes} são: análise de sentimento, detecção de \textit{spam}, e classificação de texto. Como para nosso caso (o problema de análise de sentimento), temos um vector de \textit{features} do texto e o rótulo de classe é o sentimento do texto. O conjunto de classe é um conjunto binário de positivo e negativo. Recebe um vector de \textit{features} do texto e o rótulo de classe e retorna a probabilidade de zero a um de classe, sendo o mais próximo a um o mais positivo o texto é.

\paragraph{Execução\\}

Foi criado um \textit{notebook} para a execução da tarefa. Este \textit{notebook}, contem o código do modelo \textit{Naïve Bayes} do \textit{Scikit-Learn}, e as referencias para \textit{datasets} de treino e teste. Com blocos alternados de código e \textit{markdown} que comentam a execução do modelo, foi possível executar a tentativa de classificação de sentimentos com este modelo.

As rotinas executadas foram: a criação de um objecto \textit{StemmerTokenizer}, a criação de um modelo \textit{Naïve Bayes} do \textit{Scikit-Learn}, o treinamento do modelo, em que de fazia \textit{drop} a inúmeras colunas, os testes e métricas de avaliação do modelo, a importação iterativa dos \textit{.csv} que contem os \textit{reviews} dos estabelecimentos em que iterativamente foi aplicada uma limpeza e normalização, e a execução do modelo nos \textit{reviews}, que quando classificados, geraram um arquivo \textit{.csv} com os resultados.

\paragraph{Resultados\\}

Este modelo foi capaz de classificar sentimentos com sucesso. A classificação foi bem sucedida, e foi possível classificar os sentimentos de um conjunto de \textit{reviews}, e gerar um arquivo \textit{.csv} com os resultados. Porém a classificação foi pouco precisa, e a precisão real foi baixa, apesar de ser um modelo bem sucedido e de ter sido bem treinado e avaliado. A sua \textit{accuracy} foi de entre 91\% e 87\%, dependendo dos \textit{runs}.

Calculamos que o problema está na diferença entre o português de Portugal e o português de \textit{Brasil}, o qual o ultimo é a língua dos \textit{datasets} de treino e teste, o qual o modelo foi bastante preciso; e o primeiro a língua materna dos estabelecimentos e dos clientes, o qual o modelo foi pouco preciso.

\subsubsection{\textit{Pipelines} de \textit{Transformers} do \textit{HuggingFace} e Modelos BERT da Google \textit{fine-tuned}}

Como os resultados finais do modelo \textit{Naïve Bayes} não foram satisfatórios, foi criado um \textit{pipeline} de \textit{transformers} do \textit{HuggingFace}, com um modelo BERT da Google \textit{fine-tuned} para classificar sentimentos de forma binária.

Esta biblioteca pode usar \textit{PyTorch} ou \textit{TensorFlow} como backend de computação, cada um destes tem suas vantagens e desvantagens. Por defeito ele usa \textit{PyTorch} na maioria dos seus \textit{pipelines}, mas pode ser configurado para usar \textit{TensorFlow}.

Estes \textit{pipelines} do \textit{HuggingFace} são muito mais complexos, são muito mais eficientes, e também são muito flexíveis e fáceis de usar. São definidos com um texto que determina qual é o pipeline, e os parâmetros pedidos. No nosso caso foi utilizado o pipeline de classificação de sentimentos em que tinha dois parâmetros, o \textit{tokenizer} e o modelo, mais especificamente um \textit{AutoModelForSequenceClassification} e um \textit{AutoTokenizer}, os quais foram buscar o modelo \textit{pretrained} e o \textit{tokenizer} do \href{https://huggingface.co/gchhablani/bert-base-cased-finetuned-sst2}{\texttt{gchhablani/bert-base-cased-finetuned-sst2}}

\paragraph{O que é um Transformer\\}

Um \textit{Transformer} é um tipo de rede neural que é capaz de aprender funções complexas de dados. Ele funciona através de transformar os dados de entrada em uma nova representação, que pode então ser usada para fazer predições em novos dados.

\textit{Transformers} tem muitas aplicações em linguagens de processamento natural, processamento de imagens e visão computacional. Eles foram bem-sucedidos em diversos domínios, incluindo:

\begin{itemize}
    \item Sumários de texto (\textit{Bert, DistilBert, RoBERTa, XLNet})
    \item Captação de imagens (\textit{XLNet})
    \item Tradução de imagens (\textit{Bert, DistilBert, RoBERTa})
    \item Respostas a perguntas (\textit{Bert, DistilBert, RoBERTa})
    \item \textit{Chatbots} (\textit{Bert, DistilBert, RoBERTa})
    \item Classificação (\textit{Bert, DistilBert, RoBERTa})
    \item \textit{Etc}.
\end{itemize}

O primeiro \textit{Transformer} introduzido foi o \href{https://arxiv.org/abs/1810.04805}{Bert model}. Foi desenvolvido pela \href{https://www.google.com/}{Google}. É um modelo de sequência-para-sequência que tem um \textit{encoder} e um \textit{decoder}. O \textit{encoder} mapeia uma sequência de \textit{tokens} de entrada para uma sequência de estados ocultos. O \textit{decoder} toma o output do \textit{encoder} e tenta mapeá-lo de volta para a sequência original de \textit{tokens}.

A \textit{NVIDIA} e a \textit{Facebook} desenvolveram \href{https://arxiv.org/abs/1906.08237}{XLNet} e \href{https://arxiv.org/abs/1810.04805}{DistilBert}. Eles são similares ao \textit{Bert}, mas tem uma arquitectura diferente e um conjunto de pesos diferentes. \textit{XLNet} é um modelo de sequência-para-sequência que tem um \textit{encoder} e um \textit{decoder}. O \textit{encoder} mapeia uma sequência de \textit{tokens} de entrada para uma sequência de estados ocultos. O \textit{decoder} toma o output do \textit{encoder} e tenta mapeá-lo de volta para a sequência original de \textit{tokens}. \textit{DistilBert} é similar ao \textit{XLNet}, mas é treinado em um subconjunto muito menor do que o data.

\paragraph{Como funciona um \textit{Transformer}?\\}

A funcionalidade de um \textit{Transformer} é transformar uma sequência de \textit{tokens} de entrada em uma sequência de \textit{tokens} de saída. Os \textit{tokens} de entrada são geralmente palavras, mas eles também podem ser outros tipos de \textit{tokens}, como imagens de captação. Os \textit{tokens} de saída são geralmente iguais aos \textit{tokens} de entrada, mas eles podem ser diferentes.

Ele funciona através de transformar os \textit{tokens} de entrada em uma nova representação, que pode então ser usada para fazer predições em novos dados. Os \textit{layers} de sequência são chamados de \textit{encoder} e de sequência. Elas tem x \textit{layers} de \textit{encoder} e y \textit{layers} de \textit{decoder}. x e y são geralmente iguais, mas eles podem ser diferentes. Os \textit{layers} podem ser diferentes tamanhos.

Especificamente o \textit{Transformer} é um modelo de sequência-para-sequência. Ele usa \textit{Embeddings} para os \textit{tokens} de entrada e posiciona-os antes de serem enviados para o primeiro conjunto de \textit{layers}. O primeiro conjunto de \textit{layers} tem um \textit{self-attention} mecanismo que toma os \textit{tokens} de entrada e transforma-os em uma nova representação que é adicionada e normalizada antes de ser enviada para o segundo conjunto de \textit{layers} onde os primeiros passos são iguais aos anteriores mas eles usam os outputs anteriores como inputs. Essa nova \textit{layer} combina os inputs com os outputs antigos e os outputs são enviados para o próximo conjunto de \textit{layers}. Isso é repetido até que o output seja o mesmo que o input. Então ele passa por um \textit{layer} linear \textit{regression} e um \textit{softmax}. O output é o final output.

\paragraph{\textit{Layers} em especifico\\}

Os três mais importantes tipos de \textit{layers} para bem explicar são:

\begin{itemize}
    \item A \textit{softmax function} é uma função comum em redes \textit{neurais}. Ela toma um vector e retorna um vector com o mesmo tamanho. A \textit{softmax function} é usada para normalizar o output da rede. Ela é usada para garantir que o output é uma distribuição probabilística.
    \item Uma regressão linear é uma função que toma um vector e retorna um vector. Ela é usada para fazer predições usando um meio probabilístico simples e comum.
    \item A função de \textit{self-attention} é feita usando uma combinação de regressão linear e a \textit{softmax}, com ou sem paralelismo (\textit{multi-head attention}), a função de \textit{self-attention} é usada para imprimir uma importância para o conjunto de palavras que está sendo avaliado.
\end{itemize}

\paragraph{Execução\\}

No nosso \textit{notebook}, foi criado um pipeline de \textit{transformers} do \textit{HuggingFace}, com um modelo BERT da Google \textit{fine-tuned} para classificar sentimentos de forma binária. E foi criado um \textit{notebook} para a execução da tarefa. Este \textit{notebook}, contem o código do pipeline de \textit{transformers} do \textit{HuggingFace}, e as referencias para o modelo \textit{pretrained} e o \textit{tokenizer} desse modelo.

Com blocos alternados de código e \textit{markdown} que comentam a execução do pipeline, foi possível executar a classificação de sentimentos com este pipeline. Iterativamente fomos buscar os \textit{.csv} dos \textit{reviews} dos \textit{establecimentos}, e foi aplicado o pipeline de \textit{transformers} do \textit{HuggingFace} sequencialmente a cada \textit{review}, e ao final, foi gerado um arquivo \textit{.csv} com os resultados do local.

Este pipeline foi o mais rápido das três tentativas de classificação de sentimentos, e foi o mais preciso das três tentativas de classificação de sentimentos.

\paragraph{Resultados\\}

Os resultados foram bem sucedidos, e foi possível classificar os sentimentos de um conjunto de \textit{reviews}, e gerar um arquivo \textit{.csv} com os resultados. A classificação foi bem sucedida e bastante precisa, e foi possível classificar os sentimentos de um conjunto de \textit{reviews}, e gerar um arquivo \textit{.csv} com os resultados.

De acordo com a documentação deste modelo (e do BERT original da Google), a classificação foi bem precisa, e a precisão foi de entre 97\% e 98\%. Dependendo da tarefa o BERT pode até chegar a quase 100\% de precisão, neste caso desce devido à natureza binária da classificação.

Embora não sejam 100\% precisos, o BERT foi o mais preciso das três tentativas de classificação de sentimentos e os resultados foram bastante satisfatórios.

\subsection{Ponderação dos Resultados finais}

Os resultados obtidos podem não ser 100\% precisos, mas ainda assim, podem ser bastante satisfatórios. O qual podemos considerar como um resultado final, e que pode ser usado como um indicador de qualidade do estabelecimento em avaliação.

Os resultados finais estão em pequenas tabelas de amostra nos anexos.

análise sentimental:

naive bayes explica mat por detras  (estatistica/regressão linear)
pipelines -> BERT -> Transformer (explicar que tb usam um vectorizer com um stemming/lematization e explicar como funciona a matematica de um transformer e como foi alimentado/treinado o BERT da Google)
Um transformer tal como o LSTM é um RNN (rede neural recorrente)

O extrator de palavras do YAKE tb é um naive baye

é qs igual à 2ª tentativa de analise sentimental (naive bayes) so que em vez de ser uma classificação binaria (pos / neg) gera palavras chave (keywords) (classificação multinomial)



keywords + usadas por mes
