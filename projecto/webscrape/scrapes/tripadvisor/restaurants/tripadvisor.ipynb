{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TripAdvisor\n",
    "\n",
    "### Webscraping\n",
    "\n",
    "##### Imports\n",
    "\n",
    "First we start with the imports. We need essentially three (or four) main libraries to work this out; these are:\n",
    " + requests (to fetch the website)\n",
    " + lxml (a faster html parser to speed bs4)\n",
    " + bs4 (a.k.a beautiful soup, a web scraping library)\n",
    " + pandas (a maths oriented data(set) manipulation library)\n",
    "\n",
    "Since requests uses urllib3 as a dependency, we can import it first to configure it to supress the annoying warning about the \"insecure\" connection (lack of SSL)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib3\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import pandas as pd\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Request configuration\n",
    "\n",
    "We need to configure our request, specially in this case, since TripAdvisor wont send us a webpage if we at least not try to emulate a real browser.\n",
    "First we configure our headers, ripping the main headers from our browser, as seen in the Developer tools (F12) in Chromium (we used the new Microsoft Edge).\n",
    "\n",
    "Then we request the webpage (Restaurants in Beja, Portugal) with our headers attached, a timeout to stop if it takes too long (something is wrong), and verification is disabled (SSL).\n",
    "If the status code is OK (200), doesn't print.\n",
    "\n",
    "After that we create a BeautifulSoup4 scrapable object with the html content of the page, using lxml."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    \"Access-Control-Allow-Origin\": \"*\",\n",
    "    \"Access-Control-Allow-Methods\": \"GET\",\n",
    "    \"Access-Control-Allow-Headers\": \"Content-Type\",\n",
    "    \"accept\": \"*/*\",\n",
    "    \"accept-encoding\": \"gzip, deflate, br\",\n",
    "    \"accept-language\": \"en-GB,en;q=0.9,en-US;q=0.8\",\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.45 Safari/537.36 Edg/96.0.1054.29\",\n",
    "}\n",
    "url = \"https://www.tripadvisor.pt/Restaurants-g189102-Beja_Beja_District_Alentejo.html\"\n",
    "req = requests.get(url, headers=headers, timeout=5, verify=False)\n",
    "req.status_code\n",
    "bsobj = soup(req.content, \"lxml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scraping\n",
    "\n",
    "Now we start scraping. First we start getting restaurant names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "place = []\n",
    "prelinks = []\n",
    "for name in bsobj.findAll(\"div\", {\"class\": \"OhCyu\"}):\n",
    "    place.append(re.sub(r\"\\b\\d+\\b\", \"\", name.span.text.strip())[2:])\n",
    "    prelinks.append(name.span.a[\"href\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of these will be empty, since the price is gotten via phone call.\n",
    "\n",
    "Now, we get to the weird part: Reviews. These reviews are handled by the restaurant page in weird ways:\n",
    " + only ten shown per subpage\n",
    " + each subpage is counted via a multiple of ten\n",
    " + any multiple of five non-existent will not give 404, but redirect to the first subpage\n",
    " + language selection via scraping is non-existent, no query parameters, only radio buttons with random labels, defaults chosen via domain/locale (.com .pt)\n",
    "\n",
    "So the strategy found is:\n",
    " + create a monstruos amount of subpage links (about 400)\n",
    " + scrape all reviews, even repeated via the redirect to the first subpage\n",
    " + later use sets, or dictionaries to remove duplicates\n",
    "\n",
    "That was done with this awful looking but functional code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = []\n",
    "for pre in prelinks:\n",
    "    try:\n",
    "        a = \"https://www.tripadvisor.pt\"\n",
    "        c = a + \"\" + pre\n",
    "        d = c[: (c.find(\"-Reviews-\") + len(\"-Reviews-\") - 1)]\n",
    "        e = c[(c.find(\"-Reviews-\") + len(\"-Reviews-\") - 1) :]\n",
    "        links.append(c)\n",
    "        for i in range(10, 4000, 10):\n",
    "            b = d + \"-or\" + str(i) + e\n",
    "            links.append(b)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then create the ID table of the attractions with the most basic information in a pandas DataFrame, and export that one to a .csv file that we can use in Excel, PowerBI, ML libraries like Keras, Tensorflow, SciKitLearn can use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = len(place)\n",
    "\n",
    "d1 = {\"Restaurant\": place[:length]}\n",
    "df = pd.DataFrame.from_dict(d1)\n",
    "print(df)\n",
    "df.to_csv(\"listtable.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the most terrible of codes presents you with the creations of various, separated .csv files with the scraped reviews, that we can use.\n",
    "\n",
    "It iterates all the links and since there's 400 links per restaurant, every 400 we use some list comprehension magic with sets to remove duplicates and export the DataFrame to a useful .csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "count2 = 0\n",
    "allreviews = []\n",
    "for link in links:\n",
    "    try:\n",
    "        html2 = requests.get(link, headers=headers)\n",
    "        bsobj2 = soup(html2.content, \"lxml\")\n",
    "        for r in bsobj2.findAll(\"p\", {\"class\": \"partial_entry\"}):\n",
    "            for rev in r:\n",
    "                try:\n",
    "                    rv = rev.text.strip()\n",
    "                    allreviews.append(rv + \"\\n\")\n",
    "                except:\n",
    "                    pass\n",
    "    except:\n",
    "        pass\n",
    "    count += 1\n",
    "    if count == 400:\n",
    "        seen = set()\n",
    "        allreviews = [\n",
    "            item\n",
    "            for item in allreviews\n",
    "            if not (tuple(item) in seen or seen.add(tuple(item)))\n",
    "        ]\n",
    "        dfr = pd.DataFrame.from_dict({\"Avaliações\": allreviews})\n",
    "        dfr.to_csv(\"restaurant\" + str(count2) + \".csv\")\n",
    "        print(dfr)\n",
    "        allreviews = []\n",
    "        count = 0\n",
    "        count2 += 1"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "773c6b3f1992f2b8db7c6acca0500584776d60a329f48491ffaae4707a322989"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
