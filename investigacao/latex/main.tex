\documentclass[a4paper,10pt]{article}
\usepackage[portuguese]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{newtxtext,newtxmath} %TimesNewRoman
\usepackage{verbatim}
\usepackage[a4paper,top=3cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=2cm]{geometry}
\usepackage{amsmath}
\usepackage{natbib}
\usepackage{graphicx}
\usepackage[svgnames]{xcolor}
\definecolor{ipbgreen}{RGB}{166,204,59}
\definecolor{ipbbrown}{RGB}{153,80,42}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[colorlinks=true, allcolors=ipbbrown]{hyperref}

\title{\includegraphics[scale=0.5]{ipbeja_logo.png}\\[0.5cm]Sistema de apoio à promoção do turismo rural\\Fase de Investigação} % Doc name
\author{Gonçalo Amaro -- 17440,\\ Pedro Tomás -- 18962,\\ Vítor Abreu -- 18966} % Doc's author/s
\date{11 de Novembro, 2021} % Doc date

\begin{document}

\maketitle

\newpage

{
  \hypersetup{linkcolor=black}
  \tableofcontents
}

\newpage

\section{Caracterização do objetivo}
\subsection{Objetivo do trabalho}

O objetivo deste trabalho é a analise de dados de plataformas online relacionados com turismo, para o desenvolvimento do turismo rural alentejano.
Isto é obter todos os dados de \textit{posts} e comentários relacionados com \textit{providers} de acesso, entretenimento, refeições e estadia diretamente ligados ao património cultural do alentejo.
Estes a serem analisados e classificados, criando assim um modelo de possíveis sentimentos e procuras que o comercio local tem o interesse em fornecer aos visitantes.

O nosso foco principal é nos \textit{posts} e comentários em português principalmente, com a possibilidade de analisar também o conteúdo em inglês e fazer \textit{cross-referencing}.

\subsection{Objetivo desta investigação}

Com esta investigação queremos obter o nosso curso de ação.
Quais são os websites e os métodos de obtenção de dados, e os métodos de analisar os mesmos.

\section{Sites pesquisados}

Foram usados os sites:
\begin{itemize}
    \item \href{https://www.tripadvisor.com/}{TripAdvisor}
    \item \href{https://www.booking.com/}{Booking}
    \item \href{https://www.zomato.com/}{Zomato}
    \item \href{https://developers.google.com/maps}{Google Maps}
\end{itemize}

O nosso foco principal é o \href{https://www.tripadvisor.com/}{TripAdvisor} visto que este oferece a maior variedade de conteúdo (hotéis, restaurantes e outros estabelecimentos), no entanto como uma plataforma é pouco, decidimos adicionar \href{https://www.booking.com/}{Booking} e \href{https://www.zomato.com/}{Zomato} à lista para uma maior e mais ampla rede de hotéis e restaurantes.

Foi também considerado o Google Maps, mas este apresentou um novo set de problemas que vão ser descritos já de seguida.

\section{(Im)possibilidade de uso de APIs}

Em nenhum dos sites testados foi observada uma facilidade na obtenção de acesso às suas APIs, apenas alguns (3/4) ofereceram acesso à documentação da(s) mesma(s) facilmente. A maioria requer um contacto, que foi tentado e continua sem resposta até hoje (contactos iniciados por Amaro, entre dia 26 e 29 de Out, e hoje sendo dia 11 de Nov).

Dentro dos sites que oferecem documentação foi observado que todos subdividem os seus serviços de API em 3 ou 4 APIs para casos de uso específicos (reservas, dados, etc) em vez de uma com \textit{endpoints} que oferecem solução para todos os casos.

A anomalia aqui é o Google Maps que é o único que facilita o acesso à API (mas paga, temos de ver os créditos disponíveis no \textit{Cloud Platform}), e de elevada dificuldade em \textit{scraping} pelo óbvio.

\section{Alternativas}

Sendo que é impossível o uso das APIs (que facilitariam o trabalho) temos de recorrer a outras técnicas para obter os dados.

\subsection{\textit{Web-crawling} VS \textit{web-scraping}}

\textit{Web crawling}, também conhecido como Indexação, é usado para indexar as informações na página usando bots também conhecidos como \textit{tracker}.
O \textit{trackers} é essencialmente o que os motores de busca fazem.
É uma questão de visualizar uma página como um todo e indexá-la.
Quando um Bot rastreia um site, ele passa por todas as páginas e todos os links, até a última linha do site, em busca de qualquer informação.

O \textit{web scraping}, também conhecido como extração de dados da \textit{web}, é semelhante ao \textit{web crawling}, pois identifica e localiza os dados de destino das páginas da \textit{web}.
A principal diferença é que, com o \textit{web scraping}, sabemos quem identificou o conjunto de dados exatamente, por exemplo, uma estrutura de elemento HTML para páginas da \textit{web} que estão a ser corrigidas, da qual os dados precisam ser extraídos.

Com isto visto, \textit{web scraping} é o nosso alvo, visto que minimiza lixo e é direcionado.
No entanto devemos olhar para:

\subsubsection{Vantagens de \textit{web scraping}}

\begin{enumerate}
    \item Mais rápido: É possível manusear grandes quantidades de dados que poderiam levar dias ou semanas a serem processados através do trabalho manual, com o uso do scraping podemos reduzir substancialmente o esforço e aumentar a velocidade de decisão.
    \item Confiável e consistente: Ao fazer o trabalho manual é muito fácil de haver erros, por exemplo, erros tipográficos, informações esquecidas ou inserção nas colunas erradas. O uso do \textit{web scraping} garante consistência e a qualidade dos dados.
    \item Ajuda a reduzir a carga de trabalho.
    \item Menor custo: Uma vez implementado o \textit{scraping}, o custo total da extração de dados é significativamente reduzido, especialmente quando comparado ao trabalho manual.
    \item Manutenção básica: Fazer o \textit{scraping} de dados geralmente não requer muita manutenção.
\end{enumerate}

\subsubsection{Desvantagens de \textit{web scraping}}

\begin{enumerate}
    \item Baixa proteção: Se os dados na \textit{web} são protegidos, o uso do scraping também pode se tornar um desafio e aumentar os custos.
    \item Dados estruturados: Não vai ser possível fazer scraping a 1000 sites diferentes pois cada site tem uma estrutura completamente diferente. Será necessário haver alguma estrutura básica que seja diferente em determinadas situações.
\end{enumerate}

\subsection{Necessidade de \textit{Web Scraping}}

Com o acima dito, é necessário recorrer a soluções de \textit{Web Scraping}.
A comunidade internauta reparou no mesmo, visto que em reação ao observado existem dezenas de projetos e tutoriais de Web Scraping das variadas plataformas de turismo e reservas.
Infelizmente, as mesmas não ajudam no processo e cada uma têm um forma de atuação bastante diferente.

\section{Bibliotecas de Python para \textit{Web Scraping}}

Para fazer \textit{Web Scraping} vamos usar Python, pela sua facilidade de uso e multifaceta \textit{``Development speed is more important than execution speed''}.
Com Python também temos as opções de criar cadernos Jupyter onde o próprio código e os resultados são ``encadernados'' com parágrafos de texto fazendo o próprio projeto o seu pequeno relatório de progresso e resultados; como também a criação de ambientes virtuais (\textit{containers}), onde os pacotes usados ficam registados e instalados localmente, garantindo assim a portabilidade.

Para tal linguagem existem 5 grandes bibliotecas para a resolução deste caso:
\begin{itemize}
    \item \href{https://pypi.org/project/requests/}{Requests}
    \item \href{https://pypi.org/project/BeautifulSoup/}{BeautifulSoup}
    \item \href{https://pypi.org/project/Scrapy3/}{Scrapy}
    \item \href{https://pypi.org/project/lxml/}{lxml}
\end{itemize}

Cada uma tem diferentes vantagens e desvantagens.
Caso nenhuma destas resultar, usaremos \href{https://pypi.org/project/selenium/}{Selenium} , que é uma biblioteca mais completa e poderosa que as listadas, visto que é uma ferramenta de testes de automação.
Porém tem um nível de complexidade maior e requer um setup inicial maior e mais trabalhoso, requer \textit{WebDrivers} para a execução das tarefas, pode ser complicada com Firefox, sendo preferencial usar \textit{Chromium-based Browsers} como o Chrome e o novo Edge (necessário ainda o \textit{ChromeDriver} e o \textit{EdgeDriver})..
Tentaremos evitar essa, a todo o custo, pela sua complexidade e extras desnecessários às nossas necessidades e custo temporal do setup inicial.

Para cada website pode ser necessário usar bibliotecas diferentes por necessidade ou por obtenção de informações/blog posts/etc\ldots que facilitem ou melhorem o output desejado.

\subsection{Tratamento de output}

Os outputs do conteúdo \textit{scraped} podem vir em .xml ou .csv (ou outras mas essencialmente essas duas).
Tentaremos ao máximo usar .csv, e transformar qualquer .xml em .csv, visto que um maior número de ferramentas gráficas (Excel, PowerBI, etc\ldots) e/ou bibliotecas de Python (Pandas, matplotlib, etc) para a análise de dados tratam melhor ficheiros separados por vírgulas.

Será feita também uma análise e extração de \textit{keywords} nos textos das descrições e \textit{reviews}.
Para tal existem variados algoritmos que podemos usar, alguns "clássicos" outros até de \textit{machine learning}.

Inicialmente todas as informações (dados e meta-dados) são dados como relevantes, após consideração e ponderação durante análises iniciais do decorrer do estudo poderemos descartar dados que não consideremos relevantes.
No entanto nada nos impede de tentar prever ou imaginar quais esses serão e posteriormente avaliar o nosso julgamento para ver o que foi aprendido.

\subsubsection{Informações necessárias (previsão)}

Qualquer dado diretamente relacionado com o estabelecimento como horas de funcionamento, localização, preços, classificação e quantidade de \textit{feedback}.
Também a relação das \textit{keywords} apanhadas do \textit{feedback} em relação ao oferecido.

\subsubsection{Informações a descartar (previsão)}

Provavelmente informações pessoais de clientes e empregados não nos serão minimamente importantes.

\section{Algoritmos de análise/mineração de texto}

Os algoritmos de análise de texto podem ser considerados ferramentas de mineração de texto, isto é, o processo de descoberta de conhecimento potencialmente útil e inicialmente desconhecido, ou seja, a extração de conhecimento útil utilizando bases textuais.

O processo de mineração de texto é dividido em quatro etapas bem definidas:
\begin{itemize}
    \item Seleção
    \item Pré-processamento
    \item Mineração
    \item Assimilação
\end{itemize}

Na seleção, os documentos relevantes devem ser escolhidos e mais tarde processados.
No pré-processamento ocorrerá a conversão dos documentos em uma estrutura compatível com minerador, bem como ocorrerá um tratamento especial do texto.
Na mineração, o minerador irá detetar os padrões com base no algoritmo escolhido.
E por fim, na assimilação, os utilizadores irão utilizar o conhecimento gerado para apoiar as suas decisões.

A etapa pré-processamento pode ser dividida em quatro tarefas:
\begin{itemize}
    \item Remover \textit{StopWords}
    \item Convulsão
    \item Normalização de sinónimos
    \item Indexação
\end{itemize}

Na etapa Remover \textit{stopwords} os termos com pouca ou nenhuma relevância para o documento serão removidos.
São palavras auxiliares ou conetivas, ou seja, não são discriminantes para o conteúdo do documento.

Na etapa seguinte, convulsão, realiza-se uma normalização morfológica, ou seja, realiza-se uma combinação das palavras que são variantes morfológicas em uma única forma de representação.
Um dos procedimentos mais conhecidos de convulsão é a radicalização (Stemming). Nela as palavras são reduzidas ao seu radical, ou seja, as palavras variantes morfologicamente serão combinadas em uma única representação, o radical. A radicalização pode ser efetuada com o auxílio de algoritmos de radicalização, sendo os mais utilizados o algoritmo de Porter (Porter Stemming Algorithm) e algoritmo de Orengo (Stemmer Portuguese ou RLSP).

Após a convulsão, na etapa de normalização de sinónimos, os termos que possuem significados similares serão agrupados em um único termo, por exemplo, as palavras ruído, tumulto e barulho serão substituídas ou representadas pelo termo barulho.
Na normalização de sinónimos, é formado um vocabulário controlado que se refere à utilização de termos adequados para representar um documento, sendo esses termos pré-definidos e específicos a um determinado assunto de uma área.
Isso facilita a procura, pois os termos são normalmente utilizados pelos utilizadores.

E, por fim, na etapa indexação atribui-se uma pontuação para cada termo, garantindo uma única instância do termo no documento.
No processo de atribuição de pesos devem ser considerados dois pontos:
\begin{itemize}
    \item Quanto mais vezes um termo aparece no documento, mais relevante ele é para o documento
    \item Quanto mais vezes um termo aparece na coleção de documentos, menos importante ele é para diferenciar os documentos
\end{itemize}

\subsection{Algoritmos Clássicos}

Os algoritmos clássicos são instruções passo a passo de modo a que, dada uma entrada específica, é possível rastrear e determinar exatamente a saída.
\begin{itemize}
    \item Algoritmos clássicos especificam as regras exactas para encontrar a resposta geral
    \item Um algoritmo clássico usa código e dados para prever a resposta correta para uma pergunta
    \item Algoritmo clássico produz uma saída com base nas etapas descritas no algoritmo
    \item Em algoritmos clássicos, normalmente é necessário um grande número de exemplos para determinar a que distância a equação está da equação desejada. É por isso que ``\textit{big data}'' é uma grande negócio hoje em dia
    \item Um algoritmo clássico não dá uma solução depois de chegar a uma solução optima para um problema
\end{itemize}

\subsection{Algoritmos de \textit{machine learning}}

Os algoritmos de \textit{machine learning} são partes de código que ajudam as pessoas a explorar, analisar e localizar o significado em conjuntos de dados complexos.
Cada algoritmo é um conjunto finito de instruções passo a passo inequívocas que um computador pode seguir para atingir um determinado objetivo.
Num modelo de \textit{machine learning}, o objetivo é estabelecer ou descobrir padrões que as pessoas possam utilizar para fazer previsões ou categorizar informações.

Os algoritmos de \textit{machine learning} utilizam parâmetros baseados em dados de preparação, um subconjunto de dados que representa o conjunto maior.
À medida que os dados de preparação se expandem para representar o mundo de forma mais realista, o algoritmo calcula resultados mais precisos.

Algoritmos diferentes analisam os dados de diversas formas.
Geralmente, são agrupados consoante as técnicas de \textit{machine learning} para as quais são utilizados:
\begin{itemize}
    \item Aprendizagem supervisionada
    \item Aprendizagem não supervisionada
    \item Aprendizagem por reforço
\end{itemize}

\subsubsection{Aprendizagem supervisionada}

Na aprendizagem supervisionada, os algoritmos fazem previsões com base num conjunto de exemplos etiquetados fornecidos por si.
Esta técnica é útil quando sabe como deverá ser o resultado.
Por exemplo, fornece um conjunto de dados que inclui populações de cidades por ano nos últimos 100 anos e deseja saber qual será a população de uma cidade específica dentro de quatro anos.
O resultado utiliza etiquetas que já existem no conjunto de dados: população, cidade e ano.

\subsubsection{Aprendizagem não supervisionada}

Na aprendizagem não supervisionada, os pontos de dados não são etiquetados.
O algoritmo etiqueta-os ao organizar os dados ou ao descrever a sua estrutura.
Esta técnica é útil quando não sabe como deverá ser o resultado.
Por exemplo, fornece dados de cliente e deseja criar segmentos de clientes que gostam de produtos semelhantes.
Os dados que está a fornecer não são etiquetados e as etiquetas no resultado são geradas com base nas semelhanças descobertas entre os pontos de dados.

\subsubsection{Aprendizagem de reforço}

A aprendizagem por reforço utiliza algoritmos que aprendem com resultados e decide a ação a realizar em seguida.
Após cada ação, o algoritmo recebe comentários que o ajudam a determinar se a escolha feita foi correta, neutra ou incorreta.
É uma boa técnica utilizada para sistemas automatizados que precisam de tomar muitas decisões pequenas sem orientação humana.
Por exemplo, se estivermos a criar um carro autónomo, queremos que este cumpra a lei e mantenha as pessoas seguras.
À medida que o carro ganha experiência e um histórico de reforço, aprende a permanecer dentro da faixa, a não ultrapassar o limite de velocidade e a travar quando encontrar peões.

Os algoritmos de \textit{machine learning} ajudam a responder a perguntas demasiado complexas para responder através de uma análise manual.
Existem muitos tipos diferentes de algoritmos de \textit{machine learning}.Contudo, por norma, os casos de utilização destes algoritmos enquadram-se numa destas categorias.
\begin{itemize}
    \item Algoritmos de classificação de duas classes (binários) dividem os dados em duas categorias. São úteis para perguntas com apenas duas respostas possíveis mutuamente exclusivas, incluindo perguntas de sim/não
    \item Algoritmos de classificação multiclasse (multinomial) dividem os dados em três ou mais categorias. São úteis para perguntas com três ou mais respostas possíveis mutuamente exclusivas
    \item Algoritmos de deteção de anomalias identificam os pontos de dados que estão fora dos parâmetros definidos para o que é considerado ``normal''
    \item Algoritmos de regressão preveem o valor de um novo ponto de dados com base em dados históricos
    \item Algoritmos de séries temporais mostram as alterações a um determinado valor ao longo do tempo. Com a análise e a previsão de série temporal, os dados são recolhidos a intervalos regulares ao longo do tempo e utilizados para fazer previsões e identificar tendências, sazonalidade, periodicidade e irregularidade
    \item Algoritmos de \textit{clustering} dividem os dados por vários grupos ao determinar o nível de semelhança entre os pontos de dados
    \item Algoritmos de classificação utilizam cálculos de previsão para atribuir dados a categorias predefinidas
\end{itemize}

\subsection{Decisão sobre o tipo de algoritmo}

Após a grande análise dos tipos e subtipos de algoritmos de mineração de texto (análise de texto), é óbvia a escolha em algoritmos de machine learning com aprendizagem não supervisionada para a execução da nossa análise; a questão está em qual serão usados visto que muitos deles para os nossos casos poderão ter de ser sujeitos a pré-processamento; o que removeria as nossas requeridas dimensões de análise textual.
No entanto vai continuar a haver pré-processamento como algoritmos de redução de dimensionalidade, a diferença comparado com a frase anterior é quão a pré-modelação não afetará negativamente os resultados e possíveis associações.

\subsubsection{Algoritmos considerados}

Dos variados algoritmos vistos e disponíveis na internet ou em bibliotecas de Python (como SciKitLearn, Tensorflow, Keras), demos a decisão de considerar os seguintes algoritmos como candidatos a uso e/ou pertencentes aos grupos de algoritmos usados para comparação de resultados:
\begin{itemize}
    \item LDA (Latent Dirichlet Allocation): um modelo de distribuição gaussiana, muito usado por empresas de software sobre \textit{feedback} e bug reports para associação de resultados do QA,
    \item K-Means Clustering: muito usado para fazer clusters de keywords em redes sociais,
    \item KNN (K-Nearest Neighbor): usado para agrupar dados relacionais com os contactos, relatórios, correspondência e emails em empresas,
    \item SVM (Support Vector Machines): este é usado nos mesmos lugares que regressões lineares, porém mais rápido ou poderoso, é usado para agrupar pontos como texto com imagens ou tópicos de texto em sites de vendas de 2ª mão.
\end{itemize}

No entanto existe um algoritmo de machine learning semi-supervisto (aprendizagem não supervista mas ele faz a sua auto-supervisão; logo é questão de semântica).
Este é o:
\begin{itemize}
  \item LSTM (\textit{Long-short term memory}): que é um tipo de RNN (rede neural recorrente) com ou sem um \textit{layer} CNN (um \textit{layer} de convulsão).
\end{itemize}

Poderá ser usado este método visto que não só ``está na moda'' como existe muito material de apoio por esse motivo, isto se houver tempo extra para experimentar e se essa experiência produzir melhores resultados comparativamente a um SVM por exemplo\ldots

Pode-se notar que aqui foram escolhidos algoritmos de \textit{machine learning} já comuns a grupos que realizaram tarefas semelhantes e que são algoritmos simples e rápido não sendo mais um algoritmo de uma família de algoritmos (mais complexos ou não, mas que têm muita variedade), tais como redes neurais (à exceção do RNN LSTM), algoritmos genéticos ou algoritmos lineares (como regressões lineares).

\section{Webgrafia}

Ouve uma enormidade de \textit{websites} visitados dos quais conseguimos recolher:
\begin{itemize}
    \item \href{https://www.google.com/}{Google}
    \item \href{https://www.devmedia.com.br/mineracao-de-texto-analise-comparativa-de-algoritmos-revista-sql-magazine-138/34013}{Site usado para pesquisas de algoritmos de mineração de texto}
    \item \href{https://azure.microsoft.com/pt-pt/overview/machine-learning-algorithms/#overview}{Site usado para pesquisas de algoritmos ML}
    \item \href{https://rockcontent.com/br/blog/web-crawler/}{Site usado para confirmar alguns tópicos}
    \item \href{https://www.geekboots.com/story/classic-algorithm-vs-ml-algorithm}{Site usado para retirar algumas características dos algoritmos clássicos}
\end{itemize}
\end{document}
